layout(local_size_x = 16, local_size_y = 16) in;

#define kMaxConcurrentInvocations (kSqrtMaxConcurrentInvocations*kSqrtMaxConcurrentInvocations)
#define kMaxConcurrentInvocationsMask (kMaxConcurrentInvocations-1)

#if BINNING_METHOD==0

uint getAtomicOffset() {return 0u;}

#elif BINNING_METHOD==1

uint getAtomicOffset()
{
    return gl_LocalInvocationIndex&kMaxConcurrentInvocationsMask;
}

#elif BINNING_METHOD==2

uint getAtomicOffset()
{
    return (gl_LocalInvocationIndex&63u)|((gl_WorkGroupID.x*gl_SubGroupSizeARB)&kMaxConcurrentInvocationsMask);
}

#elif BINNING_METHOD==3

#extension GL_ARB_shader_ballot: require
uint getAtomicOffset()
{
    //return gl_SubGroupInvocationARB|(((gl_WorkGroupID.x+gl_NumWorkGroups.x*(gl_WorkGroupID.y+gl_NumWorkGroups.y*gl_WorkGroupID.z))*gl_SubGroupSizeARB)&kMaxConcurrentInvocationsMask);
    return gl_SubGroupInvocationARB|((gl_WorkGroupID.x*gl_SubGroupSizeARB)&kMaxConcurrentInvocationsMask);
}

#elif BINNING_METHOD==4

#extension GL_NV_shader_thread_group: require
uint getAtomicOffset()
{
    return gl_ThreadInWarpNV+(gl_SMIDNV<<5u);
}

#elif BINNING_METHOD==5

#extension GL_ARB_shader_ballot: require
uint getAtomicOffset()
{
    return gl_SubGroupInvocationARB;
}

#elif BINNING_METHOD==6

#extension GL_NV_shader_thread_group: require
uint getAtomicOffset()
{
    return gl_SMIDNV; // why is this faster than gl_ThreadInWarpNV???
}

#endif // BINNING_METHOD


uniform uint optimizerKillerOffset;


layout(std430, binding = 0) restrict buffer OutputAtomicData {
	uint packedHistogram[];
};


//! No shared memory atomics used on purpose!

void main()
{
#if BINNING_METHOD==7
    packedHistogram[optimizerKillerOffset] = 1u;
#else
    atomicAdd(packedHistogram[optimizerKillerOffset+getAtomicOffset()],1u);
#endif // BINNING_METHOD
}

