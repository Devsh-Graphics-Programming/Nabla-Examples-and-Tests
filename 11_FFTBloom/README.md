# Bit ordering of the Nabla FFT
As you might know, the Cooley-Tukey FFT outputs the elements of a DFT in bit-reversed order. Our implementation uses the Cooley-Tukey algorithm, but the way threads swap their elements around before performing each butterfly of the FFT makes the order end up slightly different. Here's what happens for the radix-2 FFT of a 16-element array (which is what's used when you call the FFT with `ElementsPerInvocation = 2`). Please note below I use "invocation" and "thread" interchangeably.

![Radix 2 FFT](https://github.com/user-attachments/assets/8ceacf0d-d615-4b5a-9e32-3b421c70846a)

Here's how to read this diagram: Since we're working with 16 elements with 2 elements per invocation, each invocation essentially holds two elements on which it performs a butterfly at any given time. On the left we have the input array with its elements numbered from 0 to 15, and their 4-bit representations. Colours are assigned per thread: thread 0 is blue, thread 1 is green and so on. Each invocation of id `threadID` initially holds two elements, those indexed `threadID` and `threadID + WorkgroupSize`. In this case the `WorkgroupSize` is 8 (we launched 8 threads). The elements a thread holds are called `lo` and `hi`, based on which index is higher. You can at all times in the diagram tell that for each thread, in each column, there are two elements with that thread's colour. The one closest to the top is `lo`, the other is `hi` (we start counting from the top)

From the first column to the next, we perform a butterfly. After the butterfly, from the second column to the third, we need threads to swap elements so they can keep computing the FFT. You can read this in the code, but basically the first swap is done for each thread by `xor`ing its `threadID` with `stride`, which is half the distance between the elements a thread is currently holding (this `stride` starts at `WorkgroupSize / 2` and recurses by dividing by 2 at each step until it gets to 1) to get the ID of the thread it should swap values with. Then the "lowest" thread (the one for which `threadID & stride` yields 0) trades its `hi` value with the "highest" thread (the one for which the same operation yields `stride`). So for example on the third column you can see that the two blue elements are at a stride of 4 apart, after blue (thread 0) exchanges its `hi` value with pink's (thread 4 = `0 ^ 4`) `lo` value. 

For each column after that, we skip writing the butterflies (we're not intersted in that) and just keep writing which thread holds what after the exchange. You can see at the end that each thread ends up holding two consecutive elements of the output, in order: thread 0 holds the first two, thread 1 the next two, and so on. 

Now here's the catch: these outputs ARE bitreversed, it's Cooley-Tukey after all. But each thread writes its outputs to the same place it got its inputs from! So for example thread 1 grabbed elements from input positions 1 and 9. But it holds outputs indexed 2 and 3, which bitreversed become 4 and 12. So when writing to the output, this does not end up bitreversed.

For the radix-2 FFT you can see in the diagram (and it's not hard to prove) that the element of index `outputIdx` in the output holds the element of index `freqIdx` in the correctly ordered DFT, where `freqIdx` can be obtained by bitreversing all lower bits of `outputIdx`. This function also turns out to be its own inverse.

## Generalizing
Let's move to 4 Elements Per Invocation (in this case this means having 4 invocations for a 16 element FFT) to see if we notice a pattern. 

![Radix 4 FFT](https://github.com/user-attachments/assets/2c14c695-764b-4910-b556-e6ce4ee68223)

Now each thread holds 4 elements, each 4 positions apart. After the first butterfly, no swaps need to be done, since each thread will already hold the elements it needs to continue. It's important to note (since it's not clear from the diagram) that after the first column, all swaps are done in their respective halves - the top half of the element and the bottom half are computed as two independent radix 2, 8 element FFTs. This means that at the end the elements are also ordered like we expect them to: going from the top to the bottom, the elements each thread outputs are again stored in the same order it read inputs in. The output is still bitreversed, but now where each element is stored is different. It seems like we can spot a pattern though, and here it is (if you don't believe me try doing the diagram for 8 Elements Per Invocation, perhaps a 32 element FFT will be better for that):

Notice how the indices grew in the previous diagram: 0000, then 1000, then 0001, then 1001, so on. If you notice, the lower 3 bits correspond with the number of invocations, while the top bit is used to indicate which element it is locally to the thread. For example, 0|110 is the first (0) element of thead 6 (110), while 1|110 is the second element of the same thread. Something similar happens in this new diagram: we can write indices as `localIndex|threadID`. Another key thing to notice is that at the end of any diagram, what will happen is that you'll get two consecutive elements of each thread, going through every thread and then looping again to the next to elements of the first thread until you have gone through all the indices. 

You can convince yourself that this pattern holds for any amount of Elements Per Invocation and Workgroup Size (if you're cracked enough you can try sending me a proof, I'll be sure to document it).

So assuming what I said above is true, here's a good way of finding out a formula for mapping output indices to DFT indices. On one column, write all indices using the rule specified above. That would be, start with `0|0` and `1|0`. Then comes `0|1, 1|1, 0|1, 1|2` all the way up to `0|WorkgroupSize - 1, 1|WorkgroupSize - 1`. Then start again with `2|0, 3|0, 2|1, 3|1, ..., 2|WorkgroupSize - 1, 3|WorkgroupSize - 1`. And then again until you get to `ElementsPerInvocation - 2|WorkgroupSize - 1, ElementsPerInvocation - 1|WorkgroupSize - 1`. On the other column, just write all indices from 0 to the `FFTSize` ( = `WorkgroupSize * ElementsPerInvocation`) but bitreversed. This is what we write at the end of the diagram! The part that says "X holds Y".

Now we want to match these to find a rule. Here's an idea: The first column maps a number (the position in the column, counting from the top) in the range `[0, FFTSize - 1]` to an index in the output array, let's call this mapping `f` for a lack of a better word. The second column is the mapping of a number in the same range to an index in the (correctly ordered) DFT, and in fact we know this mapping to be `freqIdx -> bitreverse(freqIdx)`. Both mappings are bijective. So we're almost done! Matching the columns like we have been, we now know that for each `n` in the range, the element in the output array at index `outputIdx` given by `f(n)` holds element `bitreverse(n)` of the DFT, or `output[f(n)] = DFT[bitreverse(n)]`. Which then means that `output[outputIdx] = DFT[f^-1(bitreverse(outputIdx))]`. And similarly, `DFT[freqIdx] = output[bitreverse(f(freqIdx))]`.

So now let's get to finding a way to compute `f`!

###TODO: We already knew the mapping for 2 elements per invocation, so all of this is useful in the case of 4 or more. The expression I found so far for f is to mirror the element around the Nyquist frequency, then circular bit shift left by 1 the higher `FFT_LOG - E_LOG + 1` bits, where `FFT_LOG` is the logarithm of the size of the FFT being computed and `E_LOG` is the logarithm of the number of elements per thread. Since this expression does not work for the 2 elements per invocation case, wither find a better expression that includes it (if it's easier to compute) or leave the 2 elements per invocation case alone (template specialize it) and prove the expression for `f` for higher number of elements per invocation
