#version 430 core
#extension GL_EXT_shader_16bit_storage : require

#include "raytraceCommon.h"
layout(local_size_x = WORKGROUP_SIZE) in;

// TODO : NEE
#define MAX_RAYS_GENERATED 1
#include "raytraceCommon.glsl"

uint get_path_vertex_depth()
{
	return bitfieldExtract(pc.cummon.samplesComputed_depth,16,16);
}

#include <nbl/builtin/glsl/ext/RadeonRays/intersection.glsl>
layout(set = 3, binding = 0, std430) restrict buffer Queries
{
	nbl_glsl_ext_RadeonRays_Intersection data[];
} intersections[2];


bool get_sample_job(in uint vertex_depth_mod_2)
{
	return gl_GlobalInvocationID.x<traceIndirect[vertex_depth_mod_2].rayCount;
}

void main()
{
	const uint vertex_depth = get_path_vertex_depth();
	const uint vertex_depth_mod_2 = vertex_depth&0x1u;
	if (get_sample_job(vertex_depth_mod_2))
	{
		vec3 emissive = staticViewData.envmapBaseColor;	

		// basic reads
		const nbl_glsl_ext_RadeonRays_Intersection intersection = intersections[vertex_depth_mod_2].data[gl_GlobalInvocationID.x];
		const nbl_glsl_ext_RadeonRays_ray ray = rays[vertex_depth_mod_2].data[gl_GlobalInvocationID.x];

		const uint batchInstanceGUID = intersection.shapeid;
		const uint invalidID = 0x80000000u;
		const bool hit = batchInstanceGUID!=invalidID;

		const uvec2 outPixelLocation = unpackOutPixelLocation(ray.time);
		const vec3 throughput = vec3(
			unpackHalf2x16(ray.useless_padding[0]).rg,
			unpackHalf2x16(ray.useless_padding[1])[0]
		);
		const uint sampleID = bitfieldExtract(ray.useless_padding[1],16,16);

		//
		if (hit)
		{
			const uint triangleID = intersection.primid;
			const nbl_glsl_ext_Mitsuba_Loader_instance_data_t batchInstanceData = InstData.data[batchInstanceGUID];
			// clear the hit success flag
			intersections[vertex_depth_mod_2].data[gl_GlobalInvocationID.x].shapeid = -1;
			const uvec3 indices = get_triangle_indices(batchInstanceGUID,triangleID);
			
			// obtain ray incoming direction
			normalizedV = -mat3(batchInstanceData.normalMatrixRow0,batchInstanceData.normalMatrixRow1,batchInstanceData.normalMatrixRow2)*ray.direction;
			normalizedV = normalize(normalizedV);  // doesn't non-uniform scale screw up BxDF eval and generation?
			
			// positions
			const vec3 last_vx_pos = load_positions(indices,batchInstanceGUID);
			const vec3 geomNormal = cross(dPdBary[0],dPdBary[1]);
			const bool frontfacing = dot(geomNormal,normalizedV)>0.f;

			// get material
			const nbl_glsl_MC_oriented_material_t material = nbl_glsl_MC_material_data_t_getOriented(batchInstanceData.material,frontfacing);
			emissive = nbl_glsl_MC_oriented_material_t_getEmissive(material);
	
			const bool _continue = vertex_depth!=MAX_PATH_DEPTH && ray.maxT==FLT_MAX; // last vertex or was a NEE path
			if (_continue)
			{
				// if we ever support spatially varying emissive, we'll need to hoist barycentric computation and UV fetching to the position fetching
				const vec2 compactBary = intersection.uvwt.xy;
				
				const nbl_glsl_xoroshiro64star_state_t scramble_start_state = load_aux_vertex_attrs(compactBary,indices,batchInstanceGUID,material,outPixelLocation,vertex_depth
#ifdef TEX_PREFETCH_STREAM
					,mat2(0.0) // TODO: Covariance Rendering
#endif
				);

				const vec3 hitWorldPos = dPdBary*compactBary.xy+last_vx_pos;
				
				generate_next_rays(
					MAX_RAYS_GENERATED,batchInstanceData,material,frontfacing,vertex_depth,
					scramble_start_state,sampleID,outPixelLocation,hitWorldPos,geomNormal,throughput
				);
			}
		}
		
		// TODO: finish MIS
		vec3 acc;
		const uvec3 accumulationLocation = uvec3(outPixelLocation,sampleID%staticViewData.samplesPerPixelPerDispatch);
		const bool first_accumulating_path_vertex = accumulationLocation.z!=0u||get_path_vertex_depth()==2u;
		const bool notFirstFrame = pc.cummon.rcpFramesDispatched!=1.f;
		if (record_emission_common(acc,accumulationLocation,emissive*throughput,first_accumulating_path_vertex))
			storeAccumulation(acc,accumulationLocation);
	}
}